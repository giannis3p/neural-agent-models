{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In this notebook we train the four NN architectures that involve LSTM layers for 100x100 grid.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow==2.10.0\n",
    "pip install matplotlib==3.7.4\n",
    "pip install scipy==1.10.1\n",
    "pip install pandas==2.0.3\n",
    "pip install deepxde==1.10.1\n",
    "pip install numpy==1.24.3\n",
    "pip install scikit-learn==1.3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from tensorflow.keras.layers import LSTM, Dense, Flatten, Reshape, Conv2D\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\n",
    "from keras import backend as K\n",
    "from keras.callbacks import LearningRateScheduler, EarlyStopping, Callback\n",
    "from keras.metrics import RootMeanSquaredError\n",
    "from keras.layers import Dropout,  TimeDistributed\n",
    "from keras.regularizers import l2\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, LayerNormalization, MultiHeadAttention, Concatenate\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers, models, optimizers, losses, metrics\n",
    "tf.keras.backend.set_floatx('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seeds for reproducibility\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "\n",
    "# Configure TensorFlow for deterministic behavior\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
    "\n",
    "# Control threading for reproducibility\n",
    "tf.config.threading.set_inter_op_parallelism_threads(1)\n",
    "tf.config.threading.set_intra_op_parallelism_threads(1)\n",
    "\n",
    "# Ensure GPU determinism\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "# Define He initializer\n",
    "initializer = tf.keras.initializers.HeNormal(seed=seed_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*CUDA*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tf.config.list_physical_devices('GPU'))>0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Read data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   mcsteps  xCOM  yCOM              il8   il1   il6  il10   tnf   tgf\n",
      "0        0    78    62   [8.155469e-08]  [0.]  [0.]  [0.]  [0.]  [0.]\n",
      "1        0    29    17   [3.716848e-09]  [0.]  [0.]  [0.]  [0.]  [0.]\n",
      "2        0    64    24  [3.4276149e-09]  [0.]  [0.]  [0.]  [0.]  [0.]\n",
      "3        0    77    16  [3.7302027e-13]  [0.]  [0.]  [0.]  [0.]  [0.]\n",
      "4        0    29     7  [3.9552712e-16]  [0.]  [0.]  [0.]  [0.]  [0.]\n"
     ]
    }
   ],
   "source": [
    "sorted_concatenated_csv = \"C:/Users/Ioannis/Documents/UvA thesis/UvA-thesis/data/actual/100x100.csv\"\n",
    "data = pd.read_csv(sorted_concatenated_csv)\n",
    "#data.drop(columns=['zCOM'], inplace=True)\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Replace the 'mcsteps' column with 'time' and adjust the scale to represent 0-100 hours*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       time  xCOM  yCOM              il8              il1              il6  \\\n",
      "0         0    78    62   [8.155469e-08]             [0.]             [0.]   \n",
      "1         0    29    17   [3.716848e-09]             [0.]             [0.]   \n",
      "2         0    64    24  [3.4276149e-09]             [0.]             [0.]   \n",
      "3         0    77    16  [3.7302027e-13]             [0.]             [0.]   \n",
      "4         0    29     7  [3.9552712e-16]             [0.]             [0.]   \n",
      "...     ...   ...   ...              ...              ...              ...   \n",
      "33842   100    97    49  [1.4131037e-10]  [4.0171866e-09]  [4.0206436e-12]   \n",
      "33843   100    62    90   [9.426965e-08]    [5.53244e-09]  [2.3352102e-08]   \n",
      "33844   100     5    83   [3.467103e-11]   [1.923624e-08]   [4.866993e-11]   \n",
      "33845   100    32    17  [1.6327994e-07]  [1.9637703e-09]  [4.1984766e-10]   \n",
      "33846   100    86    51   [9.620416e-07]   [6.637888e-10]  [1.8478708e-13]   \n",
      "\n",
      "                  il10              tnf               tgf  \n",
      "0                 [0.]             [0.]              [0.]  \n",
      "1                 [0.]             [0.]              [0.]  \n",
      "2                 [0.]             [0.]              [0.]  \n",
      "3                 [0.]             [0.]              [0.]  \n",
      "4                 [0.]             [0.]              [0.]  \n",
      "...                ...              ...               ...  \n",
      "33842   [4.240001e-15]  [1.0766026e-11]   [1.4702583e-07]  \n",
      "33843  [7.4207795e-09]  [2.3492142e-08]   [4.2417334e-08]  \n",
      "33844   [6.978992e-14]  [3.8656274e-09]   [1.4110169e-08]  \n",
      "33845  [3.9633162e-12]   [4.078683e-12]  [1.17873036e-07]  \n",
      "33846   [7.918662e-18]  [3.0659374e-14]   [5.0951305e-09]  \n",
      "\n",
      "[33847 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "data['time'] = (data['mcsteps'] / 10000).astype(int)\n",
    "data = data[['time'] + [col for col in data.columns if col != 'time']]\n",
    "data.drop(columns=['mcsteps'], inplace=True)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Create 101 arrays (1 per timestep) to prepare the data to form input/output pairs for NN*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define cytokines\n",
    "cytokines = ['il8', 'il1', 'il6', 'il10', 'tnf', 'tgf']\n",
    "\n",
    "# Remove brackets and convert to float\n",
    "for col in cytokines:\n",
    "    data[col] = data[col].str.strip('[]').astype(float)\n",
    "\n",
    "# get unique time values\n",
    "unique_time = data['time'].unique()\n",
    "\n",
    "arrays = {}\n",
    "\n",
    "# iterate over unique time values\n",
    "for time in unique_time:\n",
    "    # filter data for current value of time\n",
    "    data_time = data[data['time'] == time]\n",
    "    \n",
    "    # initialize 100x100x6 array for current value of time\n",
    "    array = np.zeros((100, 100, len(cytokines)))\n",
    "    \n",
    "    # get X and Y coordinates\n",
    "    x = data_time['xCOM'].astype(int)\n",
    "    y = data_time['yCOM'].astype(int)\n",
    "    \n",
    "    # get cytokine concentrations\n",
    "    concentrations = data_time[['il8', 'il1', 'il6', 'il10', 'tnf', 'tgf']].values\n",
    "    \n",
    "    # assign cytokine concentrations to corresponding position in array\n",
    "    array[x, y, :] = concentrations\n",
    "    \n",
    "    # store array for current value of time\n",
    "    arrays[time] = array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Check for errors*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of arrays: 101\n",
      "Shape of the array: (100, 100, 6)\n",
      "Value at position (78,62): [8.155469e-08 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      " 0.000000e+00]\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of arrays:\", len(arrays))\n",
    "array = arrays[0]\n",
    "print(\"Shape of the array:\", array.shape)\n",
    "print(\"Value at position (78,62):\", array[78,62])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Construct a 5D tensor that contains a sequence of arrays to be input to the model and a 4D tensor for the output. We use sequence length of 10 here.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 10\n",
    "input_sequences = []\n",
    "output_values = []\n",
    "\n",
    "# convert dictionary values to a list of arrays\n",
    "arrays_list = [arrays[key] for key in sorted(arrays.keys())]\n",
    "\n",
    "# convert 'arrays' list to numpy array\n",
    "arrays_np = np.array(arrays_list)\n",
    "\n",
    "for i in range(len(arrays_np) - sequence_length):\n",
    "    input_seq = arrays_np[i:i+sequence_length]  # input sequence of arrays\n",
    "    output_val = arrays_np[i+sequence_length]   # array at next time step\n",
    "    \n",
    "    input_sequences.append(input_seq)\n",
    "    output_values.append(output_val)\n",
    "\n",
    "# convert lists to numpy arrays\n",
    "input_sequences = np.array(input_sequences)\n",
    "output_values = np.array(output_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Check for errors*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(91, 10, 100, 100, 6)\n",
      "(91, 100, 100, 6)\n"
     ]
    }
   ],
   "source": [
    "print(input_sequences.shape)\n",
    "print(output_values.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Metrics (rest of metrics used are pre-defined)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true)))\n",
    "\n",
    "def r_squared(y_true, y_pred):\n",
    "    SS_res =  K.sum(K.square(y_true - y_pred)) \n",
    "    SS_tot = K.sum(K.square(y_true - K.mean(y_true))) \n",
    "    return 1 - SS_res/(SS_tot + K.epsilon())\n",
    "\n",
    "def average_relative_rmse(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square((y_pred - y_true) / K.clip(K.abs(y_true), K.epsilon(), None))))\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    abs_diff = K.abs(y_true - y_pred)\n",
    "    threshold = 0.2 * y_true\n",
    "    accurate_predictions = K.less_equal(abs_diff, threshold)\n",
    "    accuracy = K.mean(accurate_predictions)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Neural Networks**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*C-LSTM: 2D convolutional layer followed by 2 LSTM layers and a FC*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr scheduler\n",
    "def lr_schedule(epoch, lr):\n",
    "    if epoch < 100:\n",
    "        return 1e-4\n",
    "    if epoch < 500:\n",
    "        return 1e-4\n",
    "    else:\n",
    "        return 1e-4\n",
    "\n",
    "# data split\n",
    "train_size = int(0.7 * input_sequences.shape[0])\n",
    "val_size = int(0.1 * input_sequences.shape[0])\n",
    "test_size = input_sequences.shape[0] - train_size - val_size\n",
    "\n",
    "X_train = input_sequences[:train_size]\n",
    "X_val = input_sequences[train_size:train_size + val_size]\n",
    "X_test = input_sequences[train_size + val_size:]\n",
    "y_train = output_values[:train_size]\n",
    "y_val = output_values[train_size:train_size + val_size]\n",
    "y_test = output_values[train_size + val_size:]\n",
    "\n",
    "# print shapes to check errors\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_val shape:\", X_val.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_val shape:\", y_val.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "\n",
    "# parameters and callbacks\n",
    "initial_lr = 1e-4\n",
    "optimizer = Adam(learning_rate=initial_lr)\n",
    "sequence_length = 10\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True)\n",
    "lr_scheduler_callback = LearningRateScheduler(lr_schedule)\n",
    "\n",
    "# model architecture\n",
    "model = Sequential([\n",
    "    Conv2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same', input_shape=(sequence_length, 100, 100, 6)),\n",
    "    Reshape((sequence_length, -1)),\n",
    "    LSTM(units=64, return_sequences=True, kernel_regularizer=l2(0.03)),\n",
    "    LSTM(units=64),\n",
    "    Dense(units=100*100*6, activation='relu'),\n",
    "    Reshape((100, 100, 6))\n",
    "])\n",
    "\n",
    "# compile\n",
    "model.compile(optimizer=optimizer, loss='mse',  metrics=[r_squared, 'mape', accuracy, average_relative_rmse, 'msle', 'mae'])\n",
    "\n",
    "# fit\n",
    "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=1000, batch_size=32, callbacks=[early_stopping, lr_scheduler_callback])\n",
    "\n",
    "# evaluate\n",
    "loss = model.evaluate(X_test, y_test)\n",
    "print(\"Test Loss:\", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*CT-LSTM: more complex architecture with maxpooling, and more neurons per layer*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lr scheduler\n",
    "def lr_schedule(epoch, lr):\n",
    "    if epoch < 100:\n",
    "        return 1e-3\n",
    "    if epoch < 400:\n",
    "        return 1e-4\n",
    "    else:\n",
    "        return 1e-5\n",
    "\n",
    "#data split\n",
    "train_size = int(0.7 * len(input_sequences))\n",
    "val_size = int(0.1 * len(input_sequences))\n",
    "\n",
    "X_train, y_train = input_sequences[:train_size], output_values[:train_size]\n",
    "X_val, y_val = input_sequences[train_size:train_size+val_size], output_values[train_size:train_size+val_size]\n",
    "X_test, y_test = input_sequences[train_size+val_size:], output_values[train_size+val_size:]\n",
    "\n",
    "# model architecture \n",
    "def build_model(input_shape):\n",
    "    input_layer = layers.Input(shape=input_shape)\n",
    "    \n",
    "    \n",
    "    x = layers.TimeDistributed(layers.Conv2D(64, (3, 3), padding='same', activation='relu'))(input_layer)\n",
    "    x = layers.TimeDistributed(layers.MaxPooling2D((2, 2)))(x)\n",
    "    x = layers.TimeDistributed(layers.Conv2D(128, (3, 3), padding='same', activation='relu'))(x)\n",
    "    x = layers.TimeDistributed(layers.MaxPooling2D((2, 2)))(x)\n",
    "    \n",
    "    x = layers.Reshape((input_shape[0], -1))(x)\n",
    "    x = layers.LSTM(128, return_sequences=True,kernel_regularizer=l2(0.01))(x)\n",
    "    x = layers.LSTM(64)(x)\n",
    "    \n",
    "    x = layers.Dense(100 * 100 * 6, activation='relu')(x)\n",
    "    output_layer = layers.Reshape((100, 100, 6))(x)\n",
    "    \n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer)\n",
    "    \n",
    "    return model\n",
    "\n",
    "#parameters and callbacks\n",
    "input_shape = (sequence_length, 100, 100, 6)\n",
    "model = build_model(input_shape)\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=150, verbose=1, restore_best_weights=True)\n",
    "initial_lr = 1e-3\n",
    "\n",
    "#compile\n",
    "model.compile(optimizer='adam', loss='mse', metrics=[r_squared, 'mape', 'accuracy', average_relative_rmse, 'msle', 'mae'])\n",
    "\n",
    "#fit\n",
    "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=1000, batch_size=16, callbacks=[lr_scheduler, early_stopping])\n",
    "\n",
    "#evaluate\n",
    "test_metrics = model.evaluate(X_test, y_test)\n",
    "print(f'Test Metrics: {test_metrics}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*STA-LSTM: NN incorporating lstm layer, spatiotemporal attention layers followed by fully connected layers*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lr scheduler\n",
    "def lr_schedule(epoch, lr):\n",
    "    if epoch < 100:\n",
    "        return 1e-6\n",
    "    if epoch < 500:\n",
    "        return 1e-3\n",
    "    else:\n",
    "        return 1e-4\n",
    "\n",
    "#model architecture\n",
    "class SpatialTemporalAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(SpatialTemporalAttention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.W_s = tf.keras.layers.Dense(hidden_size)\n",
    "        self.W_t = tf.keras.layers.Dense(hidden_size)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, lstm_output, input_data):\n",
    "        # attention weights\n",
    "        spatial_attention = tf.tanh(self.W_s(lstm_output))\n",
    "        temporal_attention = tf.tanh(self.W_t(input_data))\n",
    "        attention_scores = self.V(spatial_attention * temporal_attention)\n",
    "        attention_weights = tf.nn.softmax(attention_scores, axis=1)\n",
    "        \n",
    "        # apply attention to LSTM output\n",
    "        attended_output = tf.matmul(tf.transpose(attention_weights, [0, 2, 1]), lstm_output)\n",
    "        return attended_output\n",
    "\n",
    "class STALSTM(tf.keras.Model):\n",
    "    def __init__(self, hidden_size, input_shape):\n",
    "        super(STALSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.attention = SpatialTemporalAttention(hidden_size)\n",
    "        self.fc1 = tf.keras.layers.Dense(50, activation='relu',  kernel_regularizer=l2(0.01))\n",
    "        self.fc2 = tf.keras.layers.Dense(50, activation='relu',  kernel_regularizer=l2(0.01))\n",
    "        self.fc3 = tf.keras.layers.Dense(tf.reduce_prod(input_shape[1:]), activation='relu')\n",
    "        self.reshape = tf.keras.layers.Reshape(input_shape[1:])  # reshape to match output shape\n",
    "        self.input_shape_model = input_shape\n",
    "\n",
    "    def call(self, input_data):\n",
    "        lstm_output = self.lstm(input_data)\n",
    "        attended_output = self.attention(lstm_output, input_data)\n",
    "        x = tf.reshape(attended_output, (-1, self.hidden_size))  # flatten for fully connected layers\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        output = self.fc3(x)\n",
    "        output = self.reshape(output)  # reshape to match input shape\n",
    "        return output\n",
    "\n",
    "input_sequences_reshaped = input_sequences.reshape(input_sequences.shape[0], input_sequences.shape[1], -1)\n",
    "print(input_sequences_reshaped.shape)\n",
    "\n",
    "#data split\n",
    "train_size = int(0.7 * input_sequences_reshaped.shape[0])\n",
    "val_size = int(0.1 * input_sequences_reshaped.shape[0])\n",
    "test_size = input_sequences_reshaped.shape[0] - train_size - val_size\n",
    "\n",
    "X_train = input_sequences_reshaped[:train_size]\n",
    "X_val = input_sequences_reshaped[train_size:train_size + val_size]\n",
    "X_test = input_sequences_reshaped[train_size + val_size:]\n",
    "y_train = output_values[:train_size]\n",
    "y_val = output_values[train_size:train_size + val_size]\n",
    "y_test = output_values[train_size + val_size:]\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_val shape:\", X_val.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_val shape:\", y_val.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "\n",
    "input_shape = input_sequences.shape[1:]\n",
    "print(input_shape)\n",
    "\n",
    "model = STALSTM(hidden_size=64, input_shape=input_shape)\n",
    "\n",
    "# build the model by calling it on a batch of data\n",
    "sample_input = tf.convert_to_tensor(X_train[:1])  # take a sample batch\n",
    "_ = model(sample_input)  # calling the model on a sample input to build it\n",
    "\n",
    "#parameters and callbacks\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, restore_best_weights=True)\n",
    "initial_lr = 1e-6\n",
    "\n",
    "#compile\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=initial_lr), loss='mse',  metrics=[r_squared, 'mape', accuracy, average_relative_rmse, 'msle', 'mae'] )\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "#train\n",
    "history = model.fit(X_train, y_train, epochs=1000, batch_size=4, validation_data=(X_val, y_val), callbacks=[lr_scheduler, early_stopping])\n",
    "\n",
    "#evaluate\n",
    "loss = model.evaluate(X_test, y_test)\n",
    "print(\"Test Loss:\", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*LSTM: neural network with 3 stacked lstm layers and 4 fully connected layers*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lr scheduler\n",
    "def lr_schedule(epoch, lr):\n",
    "    if epoch < 100:\n",
    "        return 1e-2\n",
    "    if epoch < 500:\n",
    "        return 1e-3\n",
    "    else:\n",
    "        return 1e-4\n",
    "    \n",
    "#parameters and callbacks\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=150, verbose=1, restore_best_weights=True)\n",
    "\n",
    "initial_lr = 1e-2\n",
    "sequence_length =10\n",
    "\n",
    "input_sequences_reshaped = input_sequences.reshape(input_sequences.shape[0], input_sequences.shape[1], -1)\n",
    "\n",
    "#data split\n",
    "train_size = int(0.7 * input_sequences_reshaped.shape[0])\n",
    "val_size = int(0.1 * input_sequences_reshaped.shape[0])\n",
    "test_size = input_sequences_reshaped.shape[0] - train_size - val_size\n",
    "\n",
    "X_train = input_sequences_reshaped[:train_size]\n",
    "X_val = input_sequences_reshaped[train_size:train_size + val_size]\n",
    "X_test = input_sequences_reshaped[train_size + val_size:]\n",
    "y_train = output_values[:train_size]\n",
    "y_val = output_values[train_size:train_size + val_size]\n",
    "y_test = output_values[train_size + val_size:]\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_val shape:\", X_val.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_val shape:\", y_val.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "\n",
    "#model architecture\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=256, return_sequences=True, input_shape=(sequence_length, 100 * 100 * 6), kernel_regularizer=l2(0.03))) \n",
    "model.add(LSTM(units=256, return_sequences=True, input_shape=(sequence_length, 100 * 100 * 6), kernel_regularizer=l2(0.03)))\n",
    "model.add(LSTM(units=256))\n",
    "model.add(Dense(units=256, kernel_regularizer=l2(0.03)))\n",
    "model.add(Dense(units=256, kernel_regularizer=l2(0.03)))\n",
    "model.add(Dense(units=256, kernel_regularizer=l2(0.03)))\n",
    "model.add(Dense(units=100 * 100 * 6, activation='relu'))\n",
    "model.add(Reshape((100, 100, 6)))\n",
    "model.compile(optimizer=Adam(learning_rate=initial_lr), loss='mse', metrics=[r_squared, 'mape', accuracy, average_relative_rmse, 'msle', 'mae'])\n",
    "print(model.summary())\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=1000, batch_size=100, validation_data=(X_val, y_val), callbacks=[lr_scheduler, early_stopping])\n",
    "print(\"Training Loss:\", history.history['loss'])\n",
    "\n",
    "loss = model.evaluate(X_test, y_test)\n",
    "print(\"Test Loss:\", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*export in CSV the prediction for test set*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 588ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "\n",
    "y_pred_shape = y_pred.shape\n",
    "y_test_shape = y_test.shape\n",
    "\n",
    "# Flatten the y_pred and y_test tensors\n",
    "y_pred_flattened = np.reshape(y_pred, (y_pred_shape[0], -1, y_pred_shape[-1]))\n",
    "y_test_flattened = np.reshape(y_test, (y_test_shape[0], -1, y_test_shape[-1]))\n",
    "\n",
    "# Create arrays for x and y coordinates\n",
    "X = np.repeat(np.arange(y_test_shape[1]), y_test_shape[2])\n",
    "Y = np.tile(np.arange(y_test_shape[2]), y_test_shape[1])\n",
    "\n",
    "# Initialize a list to hold all data for the DataFrame\n",
    "all_data = []\n",
    "\n",
    "# Loop through each timestep and collect the data\n",
    "for timestep in range(y_pred_shape[0]):\n",
    "    y_pred_timestep = y_pred_flattened[timestep]\n",
    "    for i in range(y_pred_timestep.shape[0]):\n",
    "        data_point = {'timestep': timestep, 'X': X[i], 'Y': Y[i]}\n",
    "        data_point.update({f'feature_{j+1}': y_pred_timestep[i, j] for j in range(y_pred_shape[-1])})\n",
    "        all_data.append(data_point)\n",
    "\n",
    "# Create the DataFrame\n",
    "df_all_timesteps = pd.DataFrame(all_data)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df_all_timesteps.to_csv('data/CT-LSTM(100x100)(82-100hrs).csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Plot and export training data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = history.history['loss'][1:600]\n",
    "val_loss = history.history['val_loss'][1:600]\n",
    "\n",
    "output_dir = 'plots(100x100)/plots-CT-LSTM'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "plt.plot(train_loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.xlim(1, 600)  # Force the x-axis scale to be from 1 to 1500\n",
    "plot_filename = f'loss.png'\n",
    "plot_path = os.path.join(output_dir, plot_filename)\n",
    "plt.savefig(plot_path)\n",
    "plt.show()\n",
    "\n",
    "# Exporting the data for loss to a CSV file\n",
    "loss_data = {\n",
    "    'Epoch': list(range(1, len(train_loss) + 1)),  # Adjusting the length to match the loss data\n",
    "    'Training Loss': train_loss,\n",
    "    'Validation Loss': val_loss\n",
    "}\n",
    "loss_df = pd.DataFrame(loss_data)\n",
    "csv_filename = 'CT-LSTM-loss_data.csv'\n",
    "csv_path = os.path.join(output_dir, csv_filename)\n",
    "loss_df.to_csv(csv_path, index=False)\n",
    "\n",
    "print(f'Loss data saved to {csv_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Compute means and standard deviation and plot timeseries*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine y_test and y_pred for easier range calculation\n",
    "combined_data = np.concatenate([y_test, y_pred])\n",
    "\n",
    "output_dir = 'plots-LSTM'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Mask zeros and small values, set lower limit for log scale\n",
    "masked_data = np.ma.masked_equal(combined_data, 0)\n",
    "lower_limit = 1e-13\n",
    "\n",
    "# Calculate min and max values for each cytokine, ignoring zeros and clipping\n",
    "min_values = np.ma.min(masked_data, axis=(0, 1, 2))\n",
    "max_values = np.ma.max(masked_data, axis=(0, 1, 2))\n",
    "\n",
    "# Convert masked arrays to regular arrays with NaN where masked\n",
    "min_values = min_values.filled(np.nan)\n",
    "max_values = max_values.filled(np.nan)\n",
    "\n",
    "# Average over the spatial dimensions (X, Y coordinates)\n",
    "y_test_avg = np.mean(y_test, axis=(1, 2))\n",
    "y_pred_avg = np.mean(y_pred, axis=(1, 2))\n",
    "\n",
    "# Calculate mean and std for each cytokine across all time steps\n",
    "y_test_mean = np.mean(y_test_avg, axis=0)\n",
    "y_pred_mean = np.mean(y_pred_avg, axis=0)\n",
    "y_test_std = np.std(y_test_avg, axis=0)\n",
    "y_pred_std = np.std(y_pred_avg, axis=0)\n",
    "\n",
    "# Time steps (assuming they are from t=82 to t=100)\n",
    "time_steps = np.arange(82, 101)\n",
    "\n",
    "labels = ['il-8', 'il-1', 'il-6', 'il-10', 'tnf', 'tgf']\n",
    "\n",
    "# Plotting\n",
    "fig, axs = plt.subplots(6, 1, figsize=(12, 18), sharex=True)\n",
    "\n",
    "for i in range(6):\n",
    "    axs[i].plot(time_steps, y_test_avg[:, i], label='Actual', marker='o')\n",
    "    axs[i].plot(time_steps, y_pred_avg[:, i], label='Predicted', marker='x')\n",
    "    axs[i].set_title(f'{labels[i]} Concentration over Time\\nMean ± Std: {y_pred_mean[i]:.2e} ± {y_pred_std[i]:.2e}')\n",
    "    axs[i].set_ylabel('Concentration (log scale)')\n",
    "    axs[i].set_yscale('log')\n",
    "    axs[i].set_ylim(lower_limit, max_values[i])\n",
    "    axs[i].legend()\n",
    "\n",
    "axs[-1].set_xlabel('Time Step')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# save the plot to the specified folder\n",
    "plot_filename = f'{labels[i]}(t=82 to t=100)(time series plot).png'\n",
    "plot_path = os.path.join(output_dir, plot_filename)\n",
    "plt.savefig(plot_path)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Compute prediction accuracy (in terms of cytokines distribution on the grid) by treating this as a pseudo-classification problem*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of predictions for t=72: 61.82%\n"
     ]
    }
   ],
   "source": [
    "dataset2_path = 'data/LSTM(100x100)(72-100).csv' \n",
    "\n",
    "data2 = pd.read_csv(dataset2_path)\n",
    "\n",
    "# Define cytokines\n",
    "cytokines = ['il8', 'il1', 'il6', 'il10', 'tnf', 'tgf']\n",
    "\n",
    "# Remove brackets and convert to float in dataset 1\n",
    "#for col in cytokines:\n",
    "    #data[col] = data[col].str.strip('[]').astype(float)\n",
    "\n",
    "# Filter for time = 72\n",
    "data_72 = data[data['time'] == 100]\n",
    "data2_72 = data2[data2['timestep'] == 100]\n",
    "\n",
    "# Create a 100x100 grid representation for both datasets\n",
    "grid_size = 100\n",
    "\n",
    "def create_binary_grid(data, x_col, y_col, value_cols):\n",
    "    grid = np.zeros((grid_size, grid_size))\n",
    "    for _, row in data.iterrows():\n",
    "        x, y = int(row[x_col]), int(row[y_col])\n",
    "        if any(row[col] != 0 for col in value_cols):\n",
    "            grid[x, y] = 1\n",
    "    return grid\n",
    "\n",
    "# Create binary grids\n",
    "value_cols_data = cytokines\n",
    "value_cols_data2 = ['feature_1', 'feature_2', 'feature_3', 'feature_4', 'feature_5', 'feature_6']\n",
    "\n",
    "grid1 = create_binary_grid(data_72, 'xCOM', 'yCOM', value_cols_data)\n",
    "grid2 = create_binary_grid(data2_72, 'X', 'Y', value_cols_data2)\n",
    "\n",
    "# Calculate accuracy\n",
    "def calculate_accuracy(actual_grid, predicted_grid):\n",
    "    actual_non_zero = np.sum(actual_grid)\n",
    "    correct_predictions = np.sum((actual_grid == 1) & (predicted_grid == 1))\n",
    "    accuracy = correct_predictions / actual_non_zero if actual_non_zero > 0 else 0\n",
    "    return accuracy\n",
    "\n",
    "accuracy = calculate_accuracy(grid1, grid2)\n",
    "\n",
    "print(f\"Accuracy of predictions for t=72: {accuracy * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
